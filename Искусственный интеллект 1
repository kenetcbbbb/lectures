Вот основные тезисы из видео, выписанные кратко и без ссылок:

Введение

· Это CS50, курс «Основы искусственного интеллекта» с Брайаном Ю.
· Курс отличается от существующего "CS50 AI" тем, что он более вводный и не требует опыта в программировании. Он объясняет, как работает ИИ, его сильные и слабые стороны.

Тема лекции: Предсказания в машинном обучении

· Сегодняшняя тема — использование машинного обучения для создания предсказаний (предсказание погоды, музыки, спама и т.д.).
· В реальном мире много переменных и неопределенности, в отличие от игр с четкими правилами.

Регрессия (Prediction of Numbers)

· Задача регрессии — предсказание конкретного числа (например, как много вырастет растение в зависимости от количества солнечного света).
· Входные данные (признаки) и выходные данные (цель) можно представить в виде точек на графике, через которые можно провести линию (модель), описывающую зависимость.
· Функция потерь (Loss Function) — метрика для оценки качества предсказаний модели.
  · Абсолютная ошибка: |y(предсказанное) — y(реальное)|.
  · Квадратичная ошибка: (y(предсказанное) — y(реальное))². Она сильнее штрафует за большие ошибки.
  · Среднеквадратичная ошибка (MSE): средняя квадратичная ошибка по всем точкам данных. Цель обучения — минимизировать эту ошибку.
· Входных переменных может быть много (например, солнечный свет + питательность почвы), не только одна.

Классификация (Prediction of Categories)

· Задача классификации — отнесение объекта к какой-либо категории (например, будет дождь или нет).
· Входные данные называются признаками (features) (температура, давление, влажность).
· Метод K ближайших соседей (K-Nearest Neighbors):
  · Чтобы классифицировать новую точку, нужно посмотреть на известные точки рядом с ней.
  · Если K=1 (один ближайший сосед), новая точка получает класс этого соседа.
  · При K>1 (например, 3 или 5) соседи "голосуют", и новый объект получает класс большинства из них.
  · Метод работает в многомерных пространствах (много признаков), даже если человеку трудно это визуализировать.
  · Пример: определение спама по признакам (отправитель в контактах, орфография, длина письма).
· Машинное обучение всегда имеет погрешность, в отличие от точных вычислений калькулятора.

Нейронные сети

· Нейронные сети — мощный инструмент для классификации, вдохновленный устройством мозга.
· Перцептрон (Perceptron) — простейший тип сети.
  · Состоит из входного слоя (нейроны-признаки) и выходного нейрона.
  · Каждая связь имеет вес (weight). Чем больше вес, тем сильнее влияние входа на выход.
  · Вычисление: взвешенная сумма входов (x1w1 + x2w2 + ...) плюс смещение (bias).
  · Если результат >= 0, нейрон активируется (выдает 1), иначе — 0 (функция ступеньки).
· Процесс обучения: сеть подбирает правильные веса и смещения.
  · Изначально веса случайны (сеть ошибается).
  · Сравнивая предсказание с правильным ответом из данных, сеть корректирует веса.
  · Если сеть предсказала 0, а надо было 1 (недоактивировалась), веса увеличиваются.
  · Если предсказала 1, а надо 0 (переактивировалась), веса уменьшаются.
  · Корректировка происходит пропорционально величине входных данных.

Ограничения и развитие

· Один нейрон (перцептрон) может разделять данные только прямой линией (линейная классификация). Это не работает для сложных данных (например, задача XOR).
· Решение — скрытые слои (Hidden Layers). Слой нейронов между входом и выходом позволяет модели изучать нелинейные зависимости.
· Функции активации:
  · Ступенчатая (Step): либо 0, либо 1.
  · ReLU (Rectified Linear Unit): если вход < 0 — 0, если вход > 0 — равен входу. Это позволяет сети изучать более сложные формы, а не только прямые линии.
· Множество классов: Для классификации на несколько категорий (например, 3 вида цветов) на выходе создается несколько нейронов (по одному на класс). Их значения нормализуются в вероятности (например, 60%, 15%, 25%).
· Градиентный спуск и обратное распространение ошибки:
  · Процесс обучения: после прохода данных через сеть вычисляется ошибка, и веса корректируются на небольшую величину в правильном направлении.
  · Обратное распространение позволяет передать сигнал об ошибке от выходного слоя обратно к скрытым и входным слоям, корректируя все веса в сети.
